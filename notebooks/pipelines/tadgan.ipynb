{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from orion.data import load_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222819200</td>\n",
       "      <td>-0.366359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222840800</td>\n",
       "      <td>-0.394108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222862400</td>\n",
       "      <td>0.403625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1222884000</td>\n",
       "      <td>-0.362759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1222905600</td>\n",
       "      <td>-0.370746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp     value\n",
       "0  1222819200 -0.366359\n",
       "1  1222840800 -0.394108\n",
       "2  1222862400  0.403625\n",
       "3  1222884000 -0.362759\n",
       "4  1222905600 -0.370746"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_name = 'S-1'\n",
    "\n",
    "data = load_signal(signal_name)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from mlblocks import MLPipeline\n",
    "\n",
    "pipeline_name = 'tadgan'\n",
    "\n",
    "pipeline = MLPipeline(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'orion.primitives.tadgan.TadGAN#1': {\n",
    "        'epochs': 5,\n",
    "        'verbose': True,\n",
    "        'validation_split': 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline.set_hyperparameters(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step by step execution\n",
    "\n",
    "MLPipelines are compose of a squence of primitives, these primitives apply tranformation and calculation operations to the data and updates the variables within the pipeline. To view the primitives used by the pipeline, we access its `primtivies` attribute. \n",
    "\n",
    "The `tadgan` pipeline contains 7 primitives. we will observe how the `context` (which are the variables held within the pipeline) are updated after the execution of each primitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate',\n",
       " 'sklearn.impute.SimpleImputer',\n",
       " 'sklearn.preprocessing.MinMaxScaler',\n",
       " 'mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences',\n",
       " 'orion.primitives.tadgan.TadGAN',\n",
       " 'orion.primitives.tadgan.score_anomalies',\n",
       " 'orion.primitives.timeseries_anomalies.find_anomalies']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time segments aggregate\n",
    "this primitive creates an equi-spaced time series by aggregating values over fixed specified interval.\n",
    "\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**:\n",
    "    - `X` sequence of aggregated values, one column for each aggregation method.\n",
    "    - `index` sequence of index values (first index of each aggregated segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'index'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = pipeline.fit(data, output_=0)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry at 1222819200 has value [-0.36635895]\n",
      "entry at 1222840800 has value [-0.39410778]\n",
      "entry at 1222862400 has value [0.4036246]\n",
      "entry at 1222884000 has value [-0.36275906]\n",
      "entry at 1222905600 has value [-0.37074649]\n"
     ]
    }
   ],
   "source": [
    "for i, x in list(zip(context['index'], context['X']))[:5]:\n",
    "    print(\"entry at {} has value {}\".format(i, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer\n",
    "this primitive is an imputation transformer for filling missing values.\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**: `X` which is a transformed version of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "this primitive transforms features by scaling each feature to a given range.\n",
    "* **input**: `X` the data used to compute the per-feature minimum and maximum used for later scaling along the features axis.\n",
    "* **output**: `X` which is a transformed version of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 2\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry at 1222819200 has value [-0.36635895]\n",
      "entry at 1222840800 has value [-0.39410778]\n",
      "entry at 1222862400 has value [0.4036246]\n",
      "entry at 1222884000 has value [-0.36275906]\n",
      "entry at 1222905600 has value [-0.37074649]\n"
     ]
    }
   ],
   "source": [
    "# after scaling the data between [-1, 1]\n",
    "# in this example, no change is observed\n",
    "# since the data was pre-handedly scaled\n",
    "\n",
    "for i, x in list(zip(context['index'], context['X']))[:5]:\n",
    "    print(\"entry at {} has value {}\".format(i, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rolling window sequence\n",
    "this primitive generates many sub-sequences of the original sequence. it uses a rolling window approach to create the sub-sequences out of time series data.\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional sequence to iterate over.\n",
    "    - `index` array containing the index values of X.\n",
    "* **output**:\n",
    "    - `X` input sequences.\n",
    "    - `y` target sequences.\n",
    "    - `index` first index value of each input sequence -> renamed to `X_index`.\n",
    "    - `target_index` first index value of each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X', 'y', 'X_index', 'target_index'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 3\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (10049, 100, 1)\n",
      "y shape = (10049, 1)\n",
      "index shape = (10149,)\n",
      "target index shape = (10049,)\n"
     ]
    }
   ],
   "source": [
    "# after slicing X into multiple sub-sequences\n",
    "# we obtain a 3 dimensional matrix X where\n",
    "# the shape indicates (# slices, window size, 1)\n",
    "# and similarly y is (# slices, target size)\n",
    "\n",
    "print(\"X shape = {}\\ny shape = {}\\nindex shape = {}\\ntarget index shape = {}\".format(\n",
    "    context['X'].shape, context['y'].shape, context['index'].shape, context['target_index'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TadGAN\n",
    "this is a reconstruction based model, namely Generative Adversarial Networks (GAN), containing multiple neural networks and cycle consistency loss. the proposed model is described in the [related paper](https://arxiv.org/pdf/2009.07769.pdf).\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional array containing the input sequences for the model.\n",
    "* **output**: `y` reconstructed values -> renamed `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/sarah/opt/anaconda3/envs/orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, [Dx loss: [-2.6776628  -4.5917287   0.9348483   0.09792167]] [Dz loss: [-1.0969741  -1.4592503  -0.78547704  0.11477532]] [G loss: [ 2.9390972  -0.9107458   1.4552648   0.23945776]]\n",
      "Epoch: 2/5, [Dx loss: [-3.6530893  -5.466679    1.3018161   0.05117736]] [Dz loss: [-3.2299697  -3.0031188  -1.3129394   0.10860875]] [G loss: [ 2.799485   -1.2751291   1.7046444   0.23699693]]\n",
      "Epoch: 3/5, [Dx loss: [-3.4280794  -5.222771    1.3065965   0.04880931]] [Dz loss: [-2.2623312  -2.473338   -0.3233766   0.05343825]] [G loss: [ 1.7164162  -1.3127668   0.7973495   0.22318335]]\n",
      "Epoch: 4/5, [Dx loss: [-3.4212792  -5.419557    1.5358766   0.04624021]] [Dz loss: [-1.5228959  -0.24059121 -1.700906    0.04186013]] [G loss: [ 2.9546404  -1.5201428   2.318647    0.21561353]]\n",
      "Epoch: 5/5, [Dx loss: [-3.4299202  -5.2663007   1.4025416   0.04338401]] [Dz loss: [-0.9210284   0.37030175 -1.9520515   0.06607217]] [G loss: [ 3.5728276  -1.3929796   2.7340674   0.22317404]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X_index', 'target_index', 'X', 'y', 'y_hat', 'critic'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 4\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry at 1224979200 has value [1.], predicted value -0.4282296895980835\n",
      "entry at 1225000800 has value [-0.34946279], predicted value -0.4239683151245117\n",
      "entry at 1225022400 has value [-0.37232052], predicted value -0.44071683287620544\n",
      "entry at 1225044000 has value [1.], predicted value -0.44799137115478516\n",
      "entry at 1225065600 has value [-0.24757408], predicted value -0.45670443773269653\n"
     ]
    }
   ],
   "source": [
    "# from y_hat we compute the\n",
    "# flattened value of each \n",
    "# point using the median \n",
    "# reconstructed value\n",
    "\n",
    "def unroll_ts(y_hat):\n",
    "    predictions = list()\n",
    "    pred_length = y_hat.shape[1]\n",
    "    num_errors = y_hat.shape[1] + (y_hat.shape[0] - 1)\n",
    "\n",
    "    for i in range(num_errors):\n",
    "            intermediate = []\n",
    "\n",
    "            for j in range(max(0, i - num_errors + pred_length), min(i + 1, pred_length)):\n",
    "                intermediate.append(y_hat[i - j, j])\n",
    "\n",
    "            if intermediate:\n",
    "                predictions.append(np.median(np.asarray(intermediate)))\n",
    "\n",
    "    return np.asarray(predictions[pred_length-1:])\n",
    "\n",
    "for i, y, y_hat in list(zip(context['target_index'], context['y'], unroll_ts(context['y_hat'])))[:5]:\n",
    "    print(\"entry at {} has value {}, predicted value {}\".format(i, y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score anomalies\n",
    "\n",
    "this primitive computes an array of anomaly scores based on a combination of reconstruction error and critic output.\n",
    "\n",
    "* **input**: \n",
    "    - `y` ground truth -> renamed to `X`.\n",
    "    - `y_hat` predicted values. Each timestamp has multiple predictions.\n",
    "    - `critic` critic score. Each timestamp has multiple critic scores.  \n",
    "    - `index` time index for each `y` (start position of the window).\n",
    "* **output**: \n",
    "    - `errors` array of scores.\n",
    "    - `true_index`  time index of scores.\n",
    "    - `true` ground truth.\n",
    "    - `predictions` predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X_index', 'target_index', 'y_hat', 'critic', 'X', 'y', 'errors', 'true_index', 'true', 'predictions'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 5\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry at 1224979200 has error value 1.571\n",
      "entry at 1225000800 has error value 1.569\n",
      "entry at 1225022400 has error value 1.567\n",
      "entry at 1225044000 has error value 1.565\n",
      "entry at 1225065600 has error value 1.562\n"
     ]
    }
   ],
   "source": [
    "for i, e in list(zip(context['target_index'], context['errors']))[:5]:\n",
    "    print(\"entry at {} has error value {:.3f}\".format(i, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find anomalies\n",
    "this primitive extracts anomalies from sequences of errors following the approach explained in the [related paper](https://arxiv.org/pdf/1802.04431.pdf).\n",
    "\n",
    "* **input**: \n",
    "    - `errors` array of errors.\n",
    "    - `target_index` array of indices of errors.\n",
    "* **output**: `y` array containing start-index, end-index, score for each anomalous sequence that was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X_index', 'target_index', 'y_hat', 'critic', 'errors', 'true_index', 'true', 'predictions', 'X', 'y'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 6\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.248264e+09</td>\n",
       "      <td>1.250640e+09</td>\n",
       "      <td>0.152003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.265544e+09</td>\n",
       "      <td>1.267963e+09</td>\n",
       "      <td>0.146268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.349741e+09</td>\n",
       "      <td>1.352333e+09</td>\n",
       "      <td>0.265520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.401516e+09</td>\n",
       "      <td>1.405361e+09</td>\n",
       "      <td>2.304467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          start           end  severity\n",
       "0  1.248264e+09  1.250640e+09  0.152003\n",
       "1  1.265544e+09  1.267963e+09  0.146268\n",
       "2  1.349741e+09  1.352333e+09  0.265520\n",
       "3  1.401516e+09  1.405361e+09  2.304467"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(context['y'], columns=['start', 'end', 'severity'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion",
   "language": "python",
   "name": "orion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
